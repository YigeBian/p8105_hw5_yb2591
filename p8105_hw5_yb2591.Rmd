---
title: "p8105_hw5_yb2591"
author: "Yige Bian (yb2591)"
date: "2023-11-15"
output: github_document
---

```{r setup, include = FALSE}
Sys.setenv(LANG = "en_US")
library(tidyverse)
library(readxl)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r}
homicide_data = read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names()

homicide_data
```
The raw data `homicide_data` gathered data on homicides in 50 large U.S. cities and contains 52179 observations and 12 variables. Variables include date, information of location and victims.

The following table shows the total number of homicide and unsolved number of homicide in 51 cities
```{r}
# Create "city_state" and summarize
homicide_data =
homicide_data |>
  mutate(city_state = str_c(city,state,sep = ", "))

city_state_sum = homicide_data |>
  group_by(city_state) |>
  summarize(
    all_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest","Open/No arrest")))

city_state_sum
```

```{r}
bal_MD = city_state_sum |>
  filter(city_state == "Baltimore, MD")

bal_proptest = prop.test(
    x = pull(bal_MD, unsolved_homicides),
    n = pull(bal_MD, all_homicides)) |>
  broom::tidy()

bal_proptest |>
  select(estimate,conf.low, conf.high)
```
The estimated proportion of unsolved homicides in Baltimore, MD is `r pull(bal_proptest, estimate)`. The confidence interval is (`r pull(bal_proptest, conf.low)`, `r pull(bal_proptest, conf.high)`)

The following table shows the proportion of unsolved homicides and confidence intervals for each city
```{r}
# prop.test for all cities
all_city_proptest = city_state_sum |> 
  mutate(
    all_proptest = 
      map2(
        unsolved_homicides, all_homicides, \
        (un, all) prop.test(x = un,n = all)),
      tidy_proptest = map(all_proptest, broom::tidy)) |> 
  unnest(tidy_proptest) |> 
  select(city_state,estimate,conf.low, conf.high)

all_city_proptest
```
```{r}
# Plot to show the eatimates and Cls for each city
all_city_proptest |> 
  ggplot(aes(x = reorder(city_state,-estimate), y = estimate))+
  geom_point()+
  geom_errorbar(
    aes(
      ymin = conf.low,
      ymax = conf.high))+
  coord_flip()+
  labs(
    title = "Proportion of unsolved homicides in each city",
    x = "City",
    y = "Estimated proportion of unsolved homicides")

```
The plot above shows the estimates and CIs for each city, Tulsa, AL has the lowest estimate and the Chicago, IL has the highers estimate.

## Problem 2
```{r}
# read data for problem2
p2_df = tibble(file = list.files(path = "data/p2_data", full.names = TRUE, pattern=".csv"))

p2_df = p2_df |>
  mutate(data = map(file, read.csv))

p2_df
```

```{r}
# clean and tidy the df
tidy_p2_df = p2_df |>
  mutate(
    file = gsub('data/p2_data/', '', file),
    file = gsub('.csv', '', file)) |>
  separate(col = file, into = c('arm', 'subject_id'), sep = '_') |>
  unnest(data) |>
  pivot_longer(
    cols = starts_with("week"),
    names_to = "week",
    values_to = "data") |>
  mutate(week = as.numeric(gsub('week_', '', week)))

tidy_p2_df
```

```{r}
# plot spaghetti
tidy_p2_df |> 
  ggplot(aes(x=week, y=data, color=subject_id)) +
  geom_line()+
  facet_grid(~arm)+
  labs(
    title="Observations on each subject over 8 weeks",
    x="Week",
    y="Observations"
    )
```
The plot shows that observations in control arm have comparatively stable fluctuations, while observations in experimental arm have obvious increase from week 1 to week 8.

## Problem 3
```{r}
# fix sample size and sigma, set the seed
n = 30
sigma = 5
set.seed(1)
```

```{r}
# write the function to give mu_hat and p value
sim_mean_p = function(n, mu, sigma) {
  
  sim_data = rnorm(n, mean = mu, sd = sigma)

  result = tibble(
    mu_hat = mean(sim_data),
    p_value = pull(broom::tidy(t.test(sim_data, alternative = "two.sided", mu= 0, config.level = 0.95)),p.value)
  )}
```

The following table shows the mu_hat and p_value for 5000 iterated datasets of the model with mean of 0.
```{r}
sim_results_df = 
  expand_grid(
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(1:5000, \(i) sim_mean_p(n, 0, sigma))
  ) |> 
  unnest(estimate_df)
sim_results_df
```

The following table shows the mu_hat and p_value for 30000 iterated datasets (5000 for each value of mu) of the model with mean of 1, 2, 3, 4, 5, 6.
```{r}
sim_allmu_results_df = 
  expand_grid(
    mu = c(1, 2, 3, 4, 5, 6),
    iter = 1:5000
    ) |> 
  mutate(
    estimate_df = map(mu, \(m) sim_mean_p(n, m, sigma))
  ) |> 
  unnest(estimate_df)

sim_allmu_results_df
```
```{r}
sim_allmu_results_df |>
  mutate(if_rejected = p_value < 0.05) |>
  group_by(mu) |>
  summarize(total_rej = sum(if_rejected)) |>
  mutate(rej_prop = total_rej/5000) |>
  ggplot(aes(x = mu, y = rej_prop)) +
  geom_point() +
  geom_line() +
  labs(
    title="Power of test for each mu",
    x="True value of mu",
    y="Proportion of times H0 was rejected"
    )
```
The plot shows that the power of test increases as the increase of true value of mu.

```{r}
ave_mu_hat_all = sim_allmu_results_df |>
  group_by(mu) |>
  summarize(ave_mu_hat = mean(mu_hat))

ave_mu_hat_rej = sim_allmu_results_df |>
  filter(p_value < 0.05) |>
  group_by(mu) |>
  summarize(ave_mu_hat_rej = mean(mu_hat))

ave_mu_hat = left_join(ave_mu_hat_all, ave_mu_hat_rej, by = "mu")

ave_mu_hat |>
  pivot_longer(
    ave_mu_hat:ave_mu_hat_rej,
    names_to = "group",
    values_to = "mean") |>
  ggplot(aes(x = mu, y = mean, color = group)) + geom_point() +
  geom_line() +
  labs(
    title="Comparison of average estimated mu in all and rejected samples",
    x="True value of mu",
    y="Average estimated mu"
    ) 
```

The plot show the average estimate of mu_hat and the true value of mu for all samples and samples which null hypothesis was rejected.

We observed that the average estimated mean is larger in samples for which the null was rejected, this happened because sample means tend to be larger to be rejected, so when we consider samples for which the null was rejected, the average estimate of mu_hat tend to be larger.
